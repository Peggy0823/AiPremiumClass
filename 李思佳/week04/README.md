1.激活函数
Sigmoid 取值0-1
tanh：取值-1 ~ 1
ReLU：卷积神经网络（无负数）
Leaky ReLU: 输入为负时保留小梯度（如斜率0.01）
Swish：适合深层网络

2.归一化
公式，为了稳定训练过程，加速模型收敛并减少过拟合
重新排队，均值为0，方差为1，让所有数据统一缩放和平移，原样本特征不会受到影响，让模型分布更加稳定
BatchNorm
LayerNorm：适合长序列数据（如NLP），或批量较小的任务
GroupNorm： 平衡BN和LN,适合中等批量场景
SwitchableNorm：自适应选择归一化方式，适合动态批量场景

3.正则化  
数据筛选方法，作用是抑制过拟合，提升模型泛化能力(模型熟悉后想当然输出) 适用于更多不完整情况下模型的推理能力
dropout：随机断开神经元传递（传0值）===》》 增加模型泛化性
放在激活函数前后

4.模型结构和方法

5.优化器：
Optimizer 更新神经网络参数的工具
通常以最小化函数来改进模型的性能
a.梯度下降GD
b.批量梯度下降BGD（不适合大样本和大模型）
c.小批量梯度下降MBGD（随机打乱样本顺序，打乱组合）
d.随机梯度下降 SGD  -> 带动量的梯度下降

6.模型蒸馏